import os
import yaml
import logging
import pandas as pd
from src.db_mssql import MssqlClient
from src.db_bigquery import BigQueryClient
from src.state_manager import StateManager

# Setup Logging
logging.basicConfig(level=logging.INFO)

BQ_PROJECT = os.environ.get("BQ_PROJECT")
STATE_BUCKET = os.environ.get("STATE_BUCKET")

def load_config():
    with open("config/tables.yaml", "r") as f:
        return yaml.safe_load(f)

def clean_dataframe(df):
    # ƒê·ªïi t√™n c·ªôt __$ th√†nh cdc_
    df.columns = [col.replace('__$', 'cdc_') for col in df.columns]
    
    # Duy·ªát qua c√°c c·ªôt ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu Binary -> Hex String
    for col in df.columns:
        # L·∫•y m·∫´u d·ªØ li·ªáu d√≤ng ƒë·∫ßu ti√™n (n·∫øu c√≥)
        if len(df) > 0:
            first_val = df[col].iloc[0]
            
            # Ch·ªâ convert n·∫øu l√† bytes (varbinary/binary)
            if isinstance(first_val, bytes):
                # Apply hex() cho to√†n b·ªô c·ªôt, x·ª≠ l√Ω c·∫£ gi√° tr·ªã Null/None
                df[col] = df[col].apply(lambda x: x.hex() if isinstance(x, bytes) else x)
                
                # √âp v·ªÅ string ƒë·ªÉ ch·∫Øc ch·∫Øn (tr√°nh mixed types)
                df[col] = df[col].astype(str).replace('nan', None)

    # L∆∞u √Ω: KH√îNG convert datetime th√†nh string ·ªü ƒë√¢y.
    # ƒê·ªÉ nguy√™n object datetime ƒë·ªÉ BigQueryClient._build_schema nh·∫≠n di·ªán ƒë∆∞·ª£c.
    return df

def process_table(config, mssql, bq, state_mgr):
    table_name = config['source_table']
    bq_dataset = config['bq_dataset']
    bq_table = config['bq_table']
    pk = config['primary_key']

    logging.info(f"--- Processing {table_name} ---")

    current_max_lsn = mssql.get_max_lsn()
    table_exists = bq.check_table_exists(bq_dataset, bq_table)

    # --- INITIAL LOAD ---
    if not table_exists:
        logging.info(f"üöÄ Initial Load detected for {table_name}.")
        
        chunk_iterator = mssql.get_initial_snapshot_chunks(table_name, chunksize=100000)
        columns_schema = []
        has_data = False

        for i, chunk_df in enumerate(chunk_iterator):
            has_data = True
            chunk_df = clean_dataframe(chunk_df)
            
            if i == 0:
                columns_schema = chunk_df.columns.tolist()

            # Load Staging (Schema s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông build v√† force trong h√†m n√†y)
            bq.load_staging_chunk(chunk_df, bq_dataset, bq_table, is_first_chunk=(i==0))
            
            logging.info(f"‚úÖ Batch {i+1} loaded.")
            del chunk_df

        if has_data:
            logging.info("üì¶ Executing Merge...")
            bq.execute_merge(bq_dataset, bq_table, pk, columns_schema)
        else:
            logging.warning("‚ö†Ô∏è Source table is empty.")

    # --- INCREMENTAL LOAD ---
    else:
        start_lsn = state_mgr.get_last_lsn(bq_table)
        
        if start_lsn is None:
            logging.info("State missing. Fallback to Min LSN.")
            capture_instance = table_name.replace('.', '_')
            start_lsn = mssql.get_min_lsn(capture_instance)
            
        if start_lsn == current_max_lsn:
            logging.info("No new changes.")
            return

        logging.info(f"üîÑ Syncing changes...")
        df = mssql.get_changes(table_name, start_lsn, current_max_lsn)
        
        if df.empty:
            state_mgr.save_state(bq_table, current_max_lsn)
            return

        df = clean_dataframe(df)
        
        # Load changes v√†o Staging (V·∫´n d√πng h√†m load c≈©, n√≥ s·∫Ω t·ª± apply schema chu·∫©n)
        bq.load_staging_chunk(df, bq_dataset, bq_table, is_first_chunk=True)
        bq.execute_merge(bq_dataset, bq_table, pk, df.columns.tolist())

    state_mgr.save_state(bq_table, current_max_lsn)
    logging.info(f"üíæ Saved state.")

def main():
    configs = load_config()
    mssql = MssqlClient()
    bq = BigQueryClient(BQ_PROJECT)
    state_mgr = StateManager(STATE_BUCKET)

    for table_conf in configs['tables']:
        if table_conf.get('active', True):
            try:
                process_table(table_conf, mssql, bq, state_mgr)
            except Exception as e:
                logging.error(f"‚ùå Failed to sync {table_conf['source_table']}: {e}", exc_info=True)

if __name__ == "__main__":
    main()